{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from constants import DT, XML_DIR, START_ARM_POSE\n",
    "from constants import PUPPET_GRIPPER_POSITION_CLOSE\n",
    "from constants import PUPPET_GRIPPER_POSITION_UNNORMALIZE_FN\n",
    "from constants import PUPPET_GRIPPER_POSITION_NORMALIZE_FN\n",
    "from constants import PUPPET_GRIPPER_VELOCITY_NORMALIZE_FN\n",
    "\n",
    "from utils import sample_box_pose, sample_insertion_pose\n",
    "from dm_control import mujoco\n",
    "from dm_control.rl import control\n",
    "from dm_control.suite import base\n",
    "from dm_control import viewer\n",
    "import PIL.Image\n",
    "\n",
    "START_ARM_POSE = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimanualViperXTask(base.Task):\n",
    "    def __init__(self, random=None):\n",
    "        super().__init__(random=random)\n",
    "\n",
    "    def before_step(self, action, physics):\n",
    "        env_action = action\n",
    "        super().before_step(env_action, physics)\n",
    "        return\n",
    "\n",
    "    def initialize_episode(self, physics):\n",
    "        \"\"\"Sets the state of the environment at the start of each episode.\"\"\"\n",
    "        super().initialize_episode(physics)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_qpos(physics):\n",
    "        qpos_raw = physics.data.qpos.copy()\n",
    "        return qpos_raw[:18]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_qvel(physics):\n",
    "        qvel_raw = physics.data.qvel.copy()\n",
    "        return qvel_raw[:18]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_env_state(physics):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_observation(self, physics):\n",
    "        obs = collections.OrderedDict()\n",
    "        obs['qpos'] = self.get_qpos(physics)\n",
    "        obs['qvel'] = self.get_qvel(physics)\n",
    "        obs['env_state'] = self.get_env_state(physics)\n",
    "        obs['images'] = dict()\n",
    "        obs['images']['top'] = physics.render(height=480, width=640, camera_id='top')\n",
    "        obs['images']['angle'] = physics.render(height=480, width=640, camera_id='angle')\n",
    "        obs['images']['vis'] = physics.render(height=480, width=640, camera_id='fl_dabai')\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def get_reward(self, physics):\n",
    "        # return whether left gripper is holding the box\n",
    "        raise NotImplementedError\n",
    "\n",
    "from math import pi\n",
    "\n",
    "\n",
    "class TransferCubeTask(BimanualViperXTask):\n",
    "    def __init__(self, phi, random=None):\n",
    "        super().__init__(random=random)\n",
    "        self.max_reward = 10\n",
    "        self.phi = phi\n",
    "\n",
    "    def initialize_episode(self, physics):\n",
    "        \"\"\"Sets the state of the environment at the start of each episode.\"\"\"\n",
    "        # TODO Notice: this function does not randomize the env configuration. Instead, set BOX_POSE from outside\n",
    "        # reset qpos, control and box position\n",
    "        with physics.reset_context():\n",
    "            x, y = self.place_cube()\n",
    "            BOX_POSE = [[x, y, 0.8, 1, 0, 0, 0]]\n",
    "            physics.named.data.qpos[:18] = START_ARM_POSE\n",
    "            np.copyto(physics.data.ctrl, START_ARM_POSE)\n",
    "            assert BOX_POSE[0] is not None\n",
    "            physics.named.data.qpos[-7:] = BOX_POSE[0]\n",
    "            # print(f\"{BOX_POSE=}\")\n",
    "        super().initialize_episode(physics)\n",
    "    \n",
    "    def place_cube(self):\n",
    "        r0 = np.array([0.233, 0.3])\n",
    "        R = np.sqrt((0.52 - 0.233) ** 2 + (0.1 - 0.3) ** 2)\n",
    "        return r0 + np.array([R * np.cos(-self.phi), R * np.sin(-self.phi)])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_env_state(physics):\n",
    "        env_state = physics.data.qpos.copy()[20:]\n",
    "        return env_state\n",
    "\n",
    "    def get_reward(self, physics):\n",
    "        # return whether left gripper is holding the box\n",
    "        all_contact_pairs = []\n",
    "        for i_contact in range(physics.data.ncon):\n",
    "            id_geom_1 = physics.data.contact[i_contact].geom1\n",
    "            id_geom_2 = physics.data.contact[i_contact].geom2\n",
    "            name_geom_1 = physics.model.id2name(id_geom_1, 'geom')\n",
    "            name_geom_2 = physics.model.id2name(id_geom_2, 'geom')\n",
    "            contact_pair = (name_geom_1, name_geom_2)\n",
    "            all_contact_pairs.append(contact_pair)\n",
    "\n",
    "        touch_left_gripper = (\"red_box\", \"fl_7\") in all_contact_pairs\n",
    "        cube_touch_table = (\"red_box\", \"table2\") in all_contact_pairs\n",
    "        robot_touch_table = (\"footprint_fixed_joint_lump__box2_Link_collision_1\", \"table2\") in all_contact_pairs\n",
    "\n",
    "        reward = 0\n",
    "        if touch_left_gripper: # attempted transfer\n",
    "            reward = 3\n",
    "        if robot_touch_table: # attempted transfer\n",
    "            reward = 4\n",
    "        if cube_touch_table: # successful transfer\n",
    "            reward = 10\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0.0314159 * (-24 + 0)\n",
    "task = TransferCubeTask(phi, random=False)\n",
    "xml_path = os.path.join(XML_DIR, 'aloha_model.xml')\n",
    "physics = mujoco.Physics.from_xml_path(xml_path)\n",
    "    \n",
    "env = control.Environment(physics, task, time_limit=20, control_timestep=DT, n_sub_steps=None, flat_observation=False)\n",
    "ts = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.launch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_idx=0\n",
      "Rollout out EE space scripted policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 0.2300.\n"
     ]
    },
    {
     "ename": "PhysicsError",
     "evalue": "Physics state is invalid. Warning(s) raised: mjWARN_BADQACC",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPhysicsError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[1;32m     62\u001b[0m     action \u001b[38;5;241m=\u001b[39m pos6\n\u001b[0;32m---> 63\u001b[0m     ts \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     episode_replay\u001b[38;5;241m.\u001b[39mappend(ts)\n\u001b[1;32m     65\u001b[0m     video\u001b[38;5;241m.\u001b[39mappend(ts\u001b[38;5;241m.\u001b[39mobservation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/aloha/lib/python3.8/site-packages/dm_control/rl/control.py:106\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task\u001b[38;5;241m.\u001b[39mbefore_step(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physics)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_physics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_sub_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task\u001b[38;5;241m.\u001b[39mafter_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physics)\n\u001b[1;32m    109\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task\u001b[38;5;241m.\u001b[39mget_reward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physics)\n",
      "File \u001b[0;32m~/anaconda3/envs/aloha/lib/python3.8/site-packages/dm_control/mujoco/engine.py:176\u001b[0m, in \u001b[0;36mPhysics.step\u001b[0;34m(self, nstep)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_with_up_to_date_position_velocity(nstep)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m   mujoco\u001b[38;5;241m.\u001b[39mmj_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mptr, nstep)\n",
      "File \u001b[0;32m~/anaconda3/envs/aloha/lib/python3.8/contextlib.py:120\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aloha/lib/python3.8/site-packages/dm_control/mujoco/engine.py:344\u001b[0m, in \u001b[0;36mPhysics.check_invalid_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m message \u001b[38;5;241m=\u001b[39m _INVALID_PHYSICS_STATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    342\u001b[0m     warning_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(warning_names))\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warnings_cause_exception:\n\u001b[0;32m--> 344\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _control\u001b[38;5;241m.\u001b[39mPhysicsError(message)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarn(message)\n",
      "\u001b[0;31mPhysicsError\u001b[0m: Physics state is invalid. Warning(s) raised: mjWARN_BADQACC"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import h5py\n",
    "import cv2\n",
    "num_episodes = 50\n",
    "episode_len = 850\n",
    "render_cam_name = 'top'\n",
    "success = []\n",
    "\n",
    "for episode_idx in range(num_episodes):\n",
    "    video = []\n",
    "    print(f'{episode_idx=}')\n",
    "    print('Rollout out EE space scripted policy')\n",
    "        # setup the environment\n",
    "    phi = 0.0314159 * (-24 + episode_idx)\n",
    "    task = TransferCubeTask(phi, random=False)\n",
    "    xml_path = os.path.join(XML_DIR, 'aloha_model.xml')\n",
    "    physics = mujoco.Physics.from_xml_path(xml_path)\n",
    "    \n",
    "    env = control.Environment(physics, task, time_limit=20, control_timestep=DT, n_sub_steps=None, flat_observation=False)\n",
    "    ts = env.reset()\n",
    "    episode_replay = [ts]\n",
    "\n",
    "    pos0 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos1 = np.array([0, 0,-phi + 0.03, 1.8, 1.3, -0.9, 0, pi / 2 - phi, 0.04, 0.04, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos2 = np.array([0, 0,-phi, 1.9, 1.3, -0.9, 0, pi / 2 - phi, 0.017, 0.017, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos3 = np.array([0, 0,0, 0, 0, 0, 0, 1.57, 0.017, 0.017, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos4 = np.array([0, 0,-1.57, 0, 0, 0, 0, 1.57, 0.019, 0.019, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos5 = np.array([-1, -1,-1.57, 0, 0, 0, 0, 1.57, 0.019, 0.019, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    pos6 = np.array([1, -1,-1.57, 0, 0, 0, 0, 1.57, 0.019, 0.019, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "    for step in range(50):\n",
    "        action = pos0 * (1 - step / 50) + pos1 * (step / 50) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "\n",
    "    for step in range(50):\n",
    "        action = pos1 * (1 - step / 50) + pos2 * (step / 50) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "\n",
    "    for step in range(100):\n",
    "        action = pos2 * (1 - step / 100) + pos3 * (step / 100) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "\n",
    "    for step in range(50):\n",
    "        action = pos3 * (1 - step / 50) + pos4 * (step / 50) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "\n",
    "    for step in range(10):\n",
    "        action = pos4 * (1 - step / 10) + pos5 * (step / 10) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "    \n",
    "    for step in range(100):\n",
    "        action = pos5\n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "    \n",
    "    for step in range(10):\n",
    "        action = pos5 * (1 - step / 10) + pos6 * (step / 10) \n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "    \n",
    "    for step in range(300):\n",
    "        action = pos6\n",
    "        ts = env.step(action)\n",
    "        episode_replay.append(ts)\n",
    "        video.append(ts.observation['images'])\n",
    "\n",
    "    joint_traj = [ts.observation['qpos'] for ts in episode_replay]\n",
    "    episode_return = np.sum([ts.reward for ts in episode_replay[1:] if not (ts.reward is None)])\n",
    "    episode_max_reward = np.max([ts.reward for ts in episode_replay[1:] if not (ts.reward is None)])\n",
    "    if episode_max_reward == env.task.max_reward:\n",
    "        success.append(1)\n",
    "        print(f\"{episode_idx=} Successful, {episode_return=}\")\n",
    "    else:\n",
    "        success.append(0)\n",
    "        print(f\"{episode_idx=} Failed\")\n",
    "    data_dict = {\n",
    "            '/observations/qpos': [],\n",
    "            '/observations/qvel': [],\n",
    "            '/action': [],\n",
    "    }\n",
    "    camera_names = ['top', 'angle', 'vis']\n",
    "    for cam_name in camera_names:\n",
    "        data_dict[f'/observations/images/{cam_name}'] = []\n",
    "\n",
    "        # because the replaying, there will be eps_len + 1 actions and eps_len + 2 timesteps\n",
    "        # truncate here to be consistent\n",
    "    joint_traj = joint_traj[:-1]\n",
    "    episode_replay = episode_replay[:-1]\n",
    "        # len(joint_traj) i.e. actions: max_timesteps\n",
    "        # len(episode_replay) i.e. time steps: max_timesteps + 1\n",
    "    max_timesteps = len(joint_traj)\n",
    "    while joint_traj:\n",
    "        action = joint_traj.pop(0)\n",
    "        ts = episode_replay.pop(0)\n",
    "        data_dict['/observations/qpos'].append(ts.observation['qpos'])\n",
    "        data_dict['/observations/qvel'].append(ts.observation['qvel'])\n",
    "        data_dict['/action'].append(action)\n",
    "        for cam_name in camera_names:\n",
    "            data_dict[f'/observations/images/{cam_name}'].append(ts.observation['images'][cam_name])\n",
    "            \n",
    "    t0 = time.time()\n",
    "    dataset_dir = '/media/jacob/hdd/aloha_rollouts'\n",
    "    dataset_path = os.path.join(dataset_dir, f'episode_{episode_idx}')\n",
    "    with h5py.File(dataset_path + '.hdf5', 'w', rdcc_nbytes=1024 ** 2 * 2) as root:\n",
    "        root.attrs['sim'] = True\n",
    "        obs = root.create_group('observations')\n",
    "        image = obs.create_group('images')\n",
    "        for cam_name in camera_names:\n",
    "            _ = image.create_dataset(cam_name, (max_timesteps, 480, 640, 3), dtype='uint8',\n",
    "                                         chunks=(1, 480, 640, 3), )\n",
    "            # compression='gzip',compression_opts=2,)\n",
    "            # compression=32001, compression_opts=(0, 0, 0, 0, 9, 1, 1), shuffle=False)\n",
    "        qpos = obs.create_dataset('qpos', (max_timesteps, 18))\n",
    "        qvel = obs.create_dataset('qvel', (max_timesteps, 18))\n",
    "        action = root.create_dataset('action', (max_timesteps, 18))\n",
    "\n",
    "        for name, array in data_dict.items():\n",
    "            root[name][...] = array\n",
    "    print(f'Saving: {time.time() - t0:.1f} secs\\n')\n",
    "\n",
    "\n",
    "    cam_names = list(video[0].keys())\n",
    "    h, w, _ = video[0][cam_names[0]].shape\n",
    "    w = w * len(cam_names)\n",
    "    fps = int(1/DT)\n",
    "    out = cv2.VideoWriter(dataset_dir + '/hardcoded' + str(episode_idx) + '.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "    for ts, image_dict in enumerate(video):\n",
    "        images = []\n",
    "        for cam_name in cam_names:\n",
    "            image = image_dict[cam_name]\n",
    "            image = image[:, :, [2, 1, 0]] # swap B and R channel\n",
    "            images.append(image)\n",
    "        images = np.concatenate(images, axis=1)\n",
    "        out.write(images)\n",
    "    out.release()\n",
    "\n",
    "\n",
    "print(f'Saved to {dataset_dir}')\n",
    "print(f'Success: {np.sum(success)} / {len(success)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aloha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
